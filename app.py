# å¿…é¡»æ”¾åœ¨æœ€é¡¶è¡Œ
import gevent.monkey
gevent.monkey.patch_all()

import time
import sys
import warnings
import random
import re
import gevent
from flask import Flask, request, jsonify
from flask_socketio import SocketIO, emit
from openai import OpenAI
from colorama import init, Fore, Style

init(autoreset=True)
warnings.filterwarnings("ignore", category=DeprecationWarning)

from core.gateway import configure_gateway, manual_decrypt, manual_encrypt
from core.memory import MemoryService
from core.prompt_engine import PromptEngine
from core.llm import LLMOrchestrator
from core.tool_registry import ToolRegistry
from core.tools.builtins import GetServerTimeTool
from core.router import SemanticRouter
from core.message_logger import MessageLogger
from core.logger import FlowLogger
from core.context_manager import ContextManager
from config import Config

# å¼•å…¥ç¡¬ä»¶é©±åŠ¨ (è¿™ä¼šè‡ªåŠ¨å¯åŠ¨åå°çº¿ç¨‹)
from core.hardware.massager import massager

# ==================== æ’ä»¶ï¼šæ¶ˆæ¯åˆ‡åˆ† ====================
class MessageSplitter:
    def __init__(self, api_key, base_url, model):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model

    def split(self, text):
        prompt = (
            "ä½ æ˜¯ä¸€ä¸ªç¤¾äº¤æ¶ˆæ¯åˆ‡åˆ†ä¸“å®¶ã€‚å°†ä¸‹æ®µæ–‡å­—åˆ‡åˆ†æˆç¬¦åˆäººç±»èŠå¤©ä¹ æƒ¯çš„çŸ­å¥ã€‚\n"
            "è¦æ±‚ï¼š1.æ¯å¥è¯ç‹¬ç«‹æˆè¡Œã€‚2.ä¿æŒåŸæ„ï¼Œä¸è¦åŠ è§£é‡Šã€‚3.åˆ‡åˆ†ç‚¹è¦è‡ªç„¶ã€‚æ³¨æ„:ä¸è¦ç§è‡ªæ·»åŠ æˆ–ä¿®æ”¹å†…å®¹\n"
            f"å†…å®¹ï¼š{text}"
        )
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                timeout=10 
            )
            raw = response.choices[0].message.content.strip()
            lines = [line.strip() for line in raw.split('\n') if line.strip()]
            if len(lines) > 0: return lines
        except Exception as e:
            FlowLogger.error("Plugin", f"AIåˆ‡åˆ†å¤±è´¥ï¼Œå¯ç”¨æ­£åˆ™ä¿åº•: {e}")
        return [s.strip() for s in re.split(r'[ã€‚ï¼ï¼Ÿï¼›â€¦\n]', text) if s.strip()]

splitter = MessageSplitter(
    api_key="sk-da77abc6d7ee444396d2c0ea12f710ba",
    base_url="https://api.deepseek.com", 
    model="deepseek-chat"
)

# ==================== æœåŠ¡åˆå§‹åŒ– ====================
app = Flask(__name__)
socketio = SocketIO(
    app, 
    cors_allowed_origins=Config.CORS_ALLOWED_ORIGINS, 
    async_mode='gevent',
    ping_timeout=10,    
    ping_interval=5,
    manage_session=False 
)

configure_gateway(app)

# è¿™é‡Œä¼šåˆå§‹åŒ– ToolRegistryï¼Œè¿›è€Œå®ä¾‹åŒ– ControlMassagerTool
# ä¹‹å‰çš„æŠ¥é”™å°±æ˜¯å› ä¸ºè¿™é‡Œå®ä¾‹åŒ–å¤±è´¥ï¼Œç°åœ¨ hardware.py ä¿®å¤ååº”è¯¥æ²¡é—®é¢˜äº†
tool_registry = ToolRegistry()

raw_router_client = OpenAI(api_key=Config.ROUTER_API_KEY, base_url=Config.ROUTER_BASE_URL)
memory_service = MemoryService(cleaning_llm_client=raw_router_client)
prompt_engine = PromptEngine()
message_logger = MessageLogger() 

llm_orchestrator = LLMOrchestrator(tool_registry=tool_registry, message_logger=message_logger) 

semantic_router = SemanticRouter()
context_manager = ContextManager()
thinking_lock = False

@socketio.on('connect')
def handle_connect():
    FlowLogger.info("Socket", f"å®¢æˆ·ç«¯å·²æ¡æ‰‹è¿æ¥: {request.sid}")
    emit('conn_status', {'status': 'online', 'msg': 'å…¨åŒå·¥é€šé“å·²å¼€å¯'})

    try:
        history = message_logger.get_all_history()
        if history:
            last_msg = history[-1]
            if last_msg.get('role') == 'ai' and last_msg.get('content'):
                content = last_msg.get('content', '')
                enc_content, was_encrypted = manual_encrypt(content)
                emit('receive_message', {
                    "reply": enc_content,
                    "time": last_msg.get('time', time.time()), 
                    "is_final": True,
                    "encrypted": was_encrypted,
                    "is_sync_msg": True 
                })
                FlowLogger.info("Sync", "å·²ä»å†å²è®°å½•åŒæ­¥æœ€åä¸€æ¡ AI æ¶ˆæ¯")
    except Exception as e:
        print(f"âš ï¸ [Sync Error] åŒæ­¥æ£€æŸ¥å¤±è´¥: {e}")

@socketio.on('disconnect')
def handle_disconnect():
    FlowLogger.info("Socket", f"å®¢æˆ·ç«¯å·²æ–­å¼€: {request.sid}")

@socketio.on('send_message')
def handle_socket_chat(data):
    global thinking_lock
    thinking_lock = True 

    is_encrypted = data.get('encrypted', False)
    raw_content = data.get('content', '')

    user_input = raw_content
    if is_encrypted and Config.ENABLE_ENCRYPTION:
        user_input = manual_decrypt(raw_content)
        if user_input is None:
            FlowLogger.error("Socket", "è§£å¯†å¤±è´¥")
            emit('error', {'msg': 'è§£å¯†å¤±è´¥'})
            thinking_lock = False
            return

    FlowLogger.receive(user_input)
    message_logger.save_message("user", user_input, time.time())

    should_search = semantic_router.should_retrieve_memory(user_input)
    memory_str = ""
    if should_search:
        memories = memory_service.search_memory(user_input, top_k=Config.MEMORY_TOP_K)
        if memories:
            memory_str = "; ".join(memories)
            FlowLogger.memory("æ£€ç´¢æˆåŠŸ", f"æ‰¾åˆ° {len(memories)} æ¡ç›¸å…³è®°å¿†")

    FlowLogger.brain("æ­£åœ¨ç”Ÿæˆå®Œæ•´å›å¤...")
    messages = prompt_engine.assemble(
        user_input=user_input,
        short_term_history=context_manager.get_messages(),
        memory_context=memory_str,
        device_status="Connected"
    )
    
    full_ai_reply = llm_orchestrator.chat(messages)
    
    is_memory_saved = memory_service.add_memory(user_input, role="user") 
    context_manager.add_user_message(user_input)
    context_manager.add_ai_message(full_ai_reply)
    
    sentences = splitter.split(full_ai_reply)
    pre_think_delay = (len(sentences[0]) * 0.2) + random.uniform(0.1, 0.3)
    gevent.sleep(pre_think_delay)

    for i, sentence in enumerate(sentences):
        final_reply, was_encrypted = manual_encrypt(sentence)
        ai_ts = time.time()
        is_last_packet = (i == len(sentences) - 1)

        message_logger.save_message("ai", sentence, ai_ts)

        emit('receive_message', {
            "reply": final_reply,
            "time": ai_ts,
            "is_memory_saved": is_memory_saved if i == 0 else False,
            "encrypted": was_encrypted,
            "is_final": is_last_packet,
            "is_typing": not is_last_packet 
        })

        if not is_last_packet:
            next_len = len(sentences[i+1])
            typing_delay = (next_len * 0.2) + random.uniform(0.1, 0.3)
            gevent.sleep(typing_delay) 
    
    thinking_lock = False

@app.route('/chat', methods=['POST'])
def chat_controller():
    global thinking_lock
    data = request.get_json()
    
    is_internal = data.get('is_internal', False)
    user_input = data.get('message', '')

    if is_internal and user_input == Config.INTERNAL_TRIGGER_KEY:
        if thinking_lock:
            return jsonify({"status": "busy", "reason": "AI is talking to user"})
        
        print(f"{Fore.MAGENTA}ğŸ’“ [å¿ƒè·³] è§¦å‘ AI æ·±åº¦è‡ªçœ...{Style.RESET_ALL}")
        internal_prompt = (
            "ï¼ˆç³»ç»Ÿæç¤ºï¼šè‡ªçœæ—¶é—´ã€‚è‹¥éœ€ä¸»åŠ¨è”ç³»ç”¨æˆ·ï¼Œè¯·è°ƒç”¨ push_message_to_userã€‚ä¿æŒæ²‰é»˜åˆ™æ— éœ€æ“ä½œã€‚ï¼‰"
        )
        messages = prompt_engine.assemble(
            user_input=internal_prompt, 
            short_term_history=context_manager.get_messages(),
            device_status="Idle"
        )
        llm_orchestrator.chat(messages)
        return jsonify({"status": "heartbeat_processed"})

    FlowLogger.receive(user_input)
    messages = prompt_engine.assemble(user_input=user_input, short_term_history=context_manager.get_messages(), device_status="80%")
    ai_reply = llm_orchestrator.chat(messages)
    return jsonify({"reply": ai_reply, "time": time.time()})

@app.route('/history', methods=['GET'])
def get_history():
    history_list = message_logger.get_all_history()
    return jsonify({"history": history_list})

@app.route('/history', methods=['DELETE'])
def clear_history():
    message_logger.clear_history()
    context_manager.clear()
    FlowLogger.info("SYSTEM", "æ‰€æœ‰å†å²è®°å½•å·²æ¸…é™¤")
    return jsonify({"status": "success", "message": "äº‘ç«¯åŠä¸Šä¸‹æ–‡å·²æ¸…ç©º"})

@app.route('/status', methods=['GET'])
def server_status():
    return jsonify({"status": "online", "server_time": time.time()})

if __name__ == '__main__':
    print(f"{Fore.CYAN}ğŸŒŸ å¿ƒæ™ºç³»ç»Ÿæ‹¦æˆªå™¨å·²å°±ç»ªï¼Œç­‰å¾…å¿ƒè·³å®ˆæŠ¤è¿›ç¨‹...{Style.RESET_ALL}")
    # è¿™é‡Œä¼šå°è¯•è¿æ¥æ‰‹æŸ„
    massager._try_connect()
    socketio.run(app, host='0.0.0.0', port=8080, debug=False)