import uuid
import chromadb
from openai import OpenAI
from config import Config

class MemoryService:
    # ğŸ”¥ æ›´æ”¹å‚æ•°åç§°ï¼Œæ¥å—ä¸“é—¨çš„æ¸…æ´—å®¢æˆ·ç«¯
    def __init__(self, cleaning_llm_client=None): 
        """
        åˆå§‹åŒ–è®°å¿†æœåŠ¡
        :param cleaning_llm_client: ç”¨äºè®°å¿†æ¸…æ´—çš„ LLM å®¢æˆ·ç«¯å®ä¾‹ (ç°åœ¨ä½¿ç”¨ Router AI)
        """
        # 1. åˆå§‹åŒ–ç™¾ç‚¼ (DashScope) å®¢æˆ·ç«¯ - ç”¨äº Embedding
        self.ai_client = OpenAI(
            api_key=Config.DASHSCOPE_API_KEY,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )

        # 2. åˆå§‹åŒ– ChromaDB (æŒä¹…åŒ–å­˜å‚¨)
        self.chroma_client = chromadb.PersistentClient(path=Config.CHROMA_DB_PATH)
        
        # 3. è·å–æˆ–åˆ›å»ºé›†åˆ
        self.collection = self.chroma_client.get_or_create_collection(name="chat_history")
        
        # 4. ä¿å­˜ LLM å®¢æˆ·ç«¯ç”¨äºè®°å¿†æ¸…æ´—
        self.cleaning_llm_client = cleaning_llm_client # ğŸ”¥ ä¿å­˜ Router å®¢æˆ·ç«¯

    def _get_embedding(self, text):
        """
        ç§æœ‰æ–¹æ³•ï¼šå°†æ–‡æœ¬è½¬åŒ–ä¸ºå‘é‡
        """
        try:
            completion = self.ai_client.embeddings.create(
                model=Config.EMBEDDING_MODEL,
                input=text,
                dimensions=Config.EMBEDDING_DIM,
                encoding_format="float"
            )
            return completion.data[0].embedding
        except Exception as e:
            print(f"Embedding failed: {e}")
            return None

    def add_memory(self, text, role="user"):
        """
        æ¸…æ´—æ–‡æœ¬ï¼Œç”Ÿæˆå‘é‡å¹¶å­˜å…¥æ•°æ®åº“ã€‚
        è¿”å› True è¡¨ç¤ºæˆåŠŸå­˜å…¥ï¼ŒFalse è¡¨ç¤ºå¿½ç•¥ã€‚
        """
        final_text_to_save = text
        is_saved = False # é»˜è®¤æœªå­˜å…¥

        # === ğŸŒŸ æ™ºèƒ½æ¸…æ´—é€»è¾‘: ä½¿ç”¨ Router LLM åˆ¤æ–­è®°å¿†ä»·å€¼ ===
        if self.cleaning_llm_client and role == "user": 
            try:
                # ä» Config è·å–æ¸…æ´—æç¤ºè¯
                prompt = Config.Prompts.MEMORY_EXTRACTOR.format(input=text)
                
                # è°ƒç”¨ Router LLM
                response = self.cleaning_llm_client.chat.completions.create(
                    model=Config.ROUTER_MODEL,  
                    messages=[{"role": "user", "content": prompt}],
                    temperature=Config.ROUTER_TEMPERATURE 
                )
                result = response.choices[0].message.content.strip()

                # å¦‚æœ AI è¯´æ²¡ä»·å€¼ï¼Œç›´æ¥è·³è¿‡
                if "NO_MEMORY" in result:
                    print(f"ğŸ—‘ï¸ [è®°å¿†å±‚] å¿½ç•¥æ— æ•ˆä¿¡æ¯: {text}")
                    return False 
                
                # å¦‚æœæœ‰ä»·å€¼ï¼Œå­˜å‚¨ AI æç‚¼åçš„äº‹å®
                final_text_to_save = result
                print(f"âœ¨ [è®°å¿†å±‚] æç‚¼äº‹å®: {final_text_to_save}")

            except Exception as e:
                print(f"âš ï¸ è®°å¿†æ¸…æ´—å¤±è´¥ï¼Œé™çº§ä¸ºç›´æ¥å­˜å‚¨: {e}")
                return False 
        # ========================================

        vector = self._get_embedding(final_text_to_save)
        if vector:
            self.collection.add(
                documents=[final_text_to_save],
                embeddings=[vector],
                metadatas=[{"role": role}],
                ids=[str(uuid.uuid4())]
            )
            print(f"ğŸ’¾ [è®°å¿†å±‚] å·²å†™å…¥åº“: {final_text_to_save[:20]}...")
            is_saved = True 
            
        return is_saved

    def search_memory(self, query_text, top_k=3, threshold=0.35):
        """
        æ£€ç´¢è®°å¿†å¹¶æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
        :param query_text: ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢æ–‡æœ¬
        :param top_k: æ£€ç´¢çš„æœ€é‚»è¿‘æ¡ç›®æ•°
        :param threshold: è·ç¦»é˜ˆå€¼ã€‚ChromaDBé»˜è®¤ä½¿ç”¨L2è·ç¦»ï¼Œå€¼è¶Šå°è¶Šç›¸ä¼¼ã€‚
                          å»ºè®®èŒƒå›´ 0.3 - 0.4ã€‚
        """
        vector = self._get_embedding(query_text)
        if not vector:
            return []

        # ğŸ”¥ include å¿…é¡»åŒ…å« 'distances'
        results = self.collection.query(
            query_embeddings=[vector],
            n_results=top_k,
            include=['documents', 'distances'] 
        )

        # æå–ç»“æœåˆ—è¡¨
        documents = results.get('documents', [])[0] if results.get('documents') else []
        distances = results.get('distances', [])[0] if results.get('distances') else []

        # ğŸ”¥ ç›¸ä¼¼åº¦è¿‡æ»¤é€»è¾‘
        filtered_docs = []
        for doc, dist in zip(documents, distances):
            if dist <= threshold:
                filtered_docs.append(doc)
                print(f"âœ… [è®°å¿†æ£€ç´¢] åŒ¹é…æˆåŠŸ (è·ç¦»: {dist:.4f} <= é˜ˆå€¼: {threshold})")
            else:
                # è·ç¦»è¿‡å¤§ï¼Œåˆ¤å®šä¸ºæ— å…³å¹²æ‰°ä¿¡æ¯
                print(f"ğŸ“¡ [è®°å¿†æ£€ç´¢] è¿‡æ»¤æ— å…³æ¡ç›® (è·ç¦»: {dist:.4f} > é˜ˆå€¼: {threshold}) å†…å®¹: {doc[:20]}...")

        return filtered_docs